package constants

import (
	"time"

	"k8s.io/utils/ptr"
)

const (
	NvidiaGPUKey = "nvidia.com/gpu"
)

var (
	PendingRequeueDuration     = time.Second * 3
	StatusCheckInterval        = time.Second * 6
	GracefulPeriodSeconds      = ptr.To(int64(5))
	UnschedQueueBufferDuration = 10 * time.Second
)

const (
	// Domain is the domain prefix used for all tensor-fusion.ai related annotations and finalizers
	Domain = "tensor-fusion.ai"

	// Finalizer constants
	FinalizerSuffix = "finalizer"
	Finalizer       = Domain + "/" + FinalizerSuffix

	SchedulerName = "tensor-fusion-scheduler"

	LabelKeyOwner           = Domain + "/managed-by"
	LabelKeyClusterOwner    = Domain + "/cluster"
	LabelKeyNodeClass       = Domain + "/node-class"
	LabelKeyPodTemplateHash = Domain + "/pod-template-hash"
	LabelNodeSelectorHash   = Domain + "/node-selector-hash"
	LabelComponent          = Domain + "/component"
	// used by TF connection, for matching the related connections when worker Pod state changed
	LabelWorkerName = Domain + "/worker-name"

	ComponentClient        = "client"
	ComponentWorker        = "worker"
	ComponentHypervisor    = "hypervisor"
	ComponentNodeDiscovery = "node-discovery"
	ComponentOperator      = "operator"

	GPUNodePoolIdentifierLabelPrefix = Domain + "/pool-"
	GPUNodePoolIdentifierLabelFormat = Domain + "/pool-%s"
	NodeDeletionMark                 = Domain + "/should-delete"

	TensorFusionEnabledLabelKey = Domain + "/enabled"
	InitialGPUNodeSelector      = "nvidia.com/gpu.present=true"

	LastSyncTimeAnnotationKey = Domain + "/last-sync"
	WorkloadKey               = Domain + "/workload"

	GpuPoolKey = Domain + "/gpupool"

	// Annotation key constants
	GpuCountAnnotation      = Domain + "/gpu-count"
	TFLOPSRequestAnnotation = Domain + "/tflops-request"
	VRAMRequestAnnotation   = Domain + "/vram-request"
	TFLOPSLimitAnnotation   = Domain + "/tflops-limit"
	VRAMLimitAnnotation     = Domain + "/vram-limit"

	// StreamMultiProcessor/AICore percentage, alternative to TFLOPs request and limit, NOT Recommended way
	// NOTE: using percent will cause namespace level quota check impossible, will bypass all quota check
	// thus, percent should only be used when tenant quota is not needed, and only one type of GPU in cluster
	ComputeRequestAnnotation = Domain + "/compute-percent-request"
	ComputeLimitAnnotation   = Domain + "/compute-percent-limit"

	WorkloadProfileAnnotation = Domain + "/workload-profile"
	InjectContainerAnnotation = Domain + "/inject-container"
	IsLocalGPUAnnotation      = Domain + "/is-local-gpu"
	QoSLevelAnnotation        = Domain + "/qos"
	EmbeddedWorkerAnnotation  = Domain + "/embedded-worker"
	DedicatedWorkerAnnotation = Domain + "/dedicated-worker"
	SidecarWorkerAnnotation   = Domain + "/sidecar-worker"
	// How to isolate computing resources, default to `soft`` mode, could be `shared` or `hard`
	ComputingIsolationModeAnnotation = Domain + "/compute-isolation"
	// GPUModelAnnotation specifies the required GPU model (e.g., "A100", "H100")
	GPUModelAnnotation = Domain + "/gpu-model"
	// GPU ID list is assigned by scheduler, should not specified by user
	GPUDeviceIDsAnnotation            = Domain + "/gpu-ids"
	DedicatedGPUAnnotation            = Domain + "/dedicated-gpu"
	SetPendingOwnedWorkloadAnnotation = Domain + "/pending-owned-workload"
	PricingAnnotation                 = Domain + "/hourly-pricing"
	// In remote vGPU mode, selected workload is set by user with /workload annotation or generated by system
	SelectedWorkloadAnnotation = Domain + "/selected-workload"
	// Additional worker pod template is set by user with /worker-pod-template annotation
	WorkerPodTemplateAnnotation = Domain + "/worker-pod-template"

	WorkloadModeAnnotation = Domain + "/workload-mode"
	WorkloadModeDynamic    = "dynamic"
	WorkloadModeFixed      = "fixed"

	// no computing limit, just isolate vram memory, rely on GPU built-in time-slicing, each process gets equal share of GPU
	// Pros: simple and stable, no performance overhead, maximize GPU utilization when well-scheduled
	// Cons: can not auto-scale and differentiate QoS levels, TFLOPs limit does not take effect, may cause resource contention
	ComputingIsolationModeShared = "shared"

	// default isolation mode, use Proportional-Integral-Derivative controller to isolate computing resources and assign time slices
	// Pros: can set QoS levels for different workloads, TFLOPs limit is relatively accurate
	// Cons: ~1% performance overhead, when burst credits are consumed,
	ComputingIsolationModeSoft = "soft"

	// use SM partitioning to isolate computing resources, each Pod get dedicated SMs depends on GPU driver support
	// Pros: better performance isolation, no performance overhead
	// Cons: can not auto-scale dynamically, percent may not 1%/1TFLOPs accuracy, coupled with GPU vendor's SM partitioning implementation
	// NOTE: this can only be used in Remote or Local+SidecarWorker mode, not supported in LocalGPU mode (because no TensorFusion Worker)
	ComputingIsolationModeHard = "hard"

	// Annotations for killer switch: disable features
	// ['gpu-opt', 'mem-manager', 'gpu-limiter']
	DisableFeaturesAnnotation = Domain + "/disable-features"
	BuiltInFeaturesGpuOpt     = "gpu-opt"
	BuiltInFeaturesGpuLimiter = "gpu-limiter"
	BuiltInFeaturesMemManager = "mem-manager"
	// For debug purpose only of Remote vGPU, disable start worker to manual start with ad-hoc command inside Pod
	BuiltInFeatureStartWorker = "start-worker"

	GenHostPortLabel        = Domain + "/host-port"
	GenHostPortLabelValue   = "auto"
	GenHostPortNameLabel    = Domain + "/port-name"
	GenPortNumberAnnotation = Domain + "/port-number"

	AutoScaleResourcesAnnotation      = Domain + "/auto-resources"
	AutoScaleReplicasAnnotation       = Domain + "/auto-replicas"
	AutoScaleTargetResourceAnnotation = Domain + "/auto-scale-target-resource"

	GpuReleasedAnnotation = Domain + "/gpu-released"

	TensorFusionPodCounterKeyAnnotation = Domain + "/pod-counter-key"
	TensorFusionPodCountAnnotation      = Domain + "/tf-pod-count"
	TensorFusionWorkerSuffix            = "-tf"

	// For grey release
	TensorFusionEnabledReplicasAnnotation = Domain + "/enabled-replicas"
	TensorFusionDefaultPoolKeyAnnotation  = Domain + "/is-default-pool"
	// Eviction protection annotation for controlling pod eviction timing
	EvictionProtectionAnnotation = Domain + "/eviction-protection"

	NamespaceDefaultVal = "tensor-fusion-sys"

	KubernetesHostNameLabel = "kubernetes.io/hostname"
	KarpenterExpansionLabel = Domain + "/expansion-source"

	HypervisorServiceAccountName = "tensor-fusion-hypervisor-sa"

	TSDBVersionConfigMap = "tensor-fusion-tsdb-version"

	QoSLevelLow      = "low"
	QoSLevelMedium   = "medium"
	QoSLevelHigh     = "high"
	QoSLevelCritical = "critical"
)

// for avoid golang lint issues
const (
	TrueStringValue  = "true"
	FalseStringValue = "false"
)

const (
	ConditionStatusTypeReady           = "Ready"
	ConditionStatusTypeGPUScheduled    = "GPUScheduled"
	ConditionStatusTypeConnectionReady = "ConnectionReady"
	ConditionStatusTypeNodeProvisioned = "NodeProvisioned"
	ConditionStatusTypePoolReady       = "PoolReady"

	ConditionStatusTypeGPUPool               = "GPUPoolReady"
	ConditionStatusTypeTimeSeriesDatabase    = "TimeSeriesDatabaseReady"
	ConditionStatusTypeCloudVendorConnection = "CloudVendorConnectionReady"

	ConditionStatusTypeRecommendationProvided = "RecommendationProvided"
)

const (
	PhaseUnknown    = "Unknown"
	PhasePending    = "Pending"
	PhaseUpdating   = "Updating"
	PhaseScheduling = "Scheduling"
	PhaseMigrating  = "Migrating"
	PhaseDestroying = "Destroying"

	PhaseRunning   = "Running"
	PhaseSucceeded = "Succeeded"
	PhaseFailed    = "Failed"
)

const (
	// No disrupt label, similar to Karpenter, avoid TFConnection/Worker/GPUNode to be moved to another node or destroying node.
	// Refer: https://karpenter.sh/docs/concepts/disruption/
	SchedulingDoNotDisruptLabel = Domain + "/do-not-disrupt"
)

const (
	GPUNodeOSLinux   = "linux"
	GPUNodeOSWindows = "windows"
	GPUNodeOSMacOS   = "macos"
)

// To match GPUNode with K8S node, when creating from cloud vendor, must set a label from cloud-init userdata
const (
	ProvisionerLabelKey        = Domain + "/node-provisioner"
	ProvisionerMissingLabel    = Domain + "/orphan"
	ProvisionerNamePlaceholder = "__GPU_NODE_RESOURCE_NAME__"
)

const TFDataPath = "/run/tensor-fusion"
const TFDataPathWorkerExpr = "shm/$(POD_NAMESPACE)/$(POD_NAME)"
const DataVolumeName = "tf-data"
const TransportShmVolumeName = "tf-transport-shm"
const TransportShmPath = "/dev/shm"
const TensorFusionPoolManualCompaction = Domain + "/manual-compaction"
const TensorFusionSystemName = "tensor-fusion"

const (
	LeaderInfoConfigMapName        = "tensor-fusion-operator-leader-info"
	LeaderInfoConfigMapLeaderIPKey = "leader-ip"
)

const ShortUUIDAlphabet = "123456789abcdefghijkmnopqrstuvwxy"
const SpotInstanceAssumedDiscountRatio = 0.3

const (
	LowFrequencyObjFailureInitialDelay        = 300 * time.Millisecond
	LowFrequencyObjFailureMaxDelay            = 1000 * time.Second
	LowFrequencyObjFailureMaxRPS              = 1
	LowFrequencyObjFailureMaxBurst            = 1
	LowFrequencyObjFailureConcurrentReconcile = 5
)

const GiBToBytes = 1024 * 1024 * 1024

const AuthorizationHeader = "Authorization"
const ExtraVerificationInfoPodIDKey = "authentication.kubernetes.io/pod-uid"

const SchedulerSimulationKey = "simulate-schedule"

const MobileGpuClockSpeedMultiplier = 0.75
const DefaultEvictionProtectionPriceRatio = 1.2
const NodeCriticalPriorityClassName = "system-node-critical"
const KarpenterNodeClaimKind = "NodeClaim"
const KarpenterNodePoolKind = "NodePool"
