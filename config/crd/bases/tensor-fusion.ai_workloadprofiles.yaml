---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.16.4
  name: workloadprofiles.tensor-fusion.ai
spec:
  group: tensor-fusion.ai
  names:
    kind: WorkloadProfile
    listKind: WorkloadProfileList
    plural: workloadprofiles
    singular: workloadprofile
  scope: Namespaced
  versions:
  - name: v1
    schema:
      openAPIV3Schema:
        description: WorkloadProfile is the Schema for the workloadprofiles API.
        properties:
          apiVersion:
            description: |-
              APIVersion defines the versioned schema of this representation of an object.
              Servers should convert recognized schemas to the latest internal value, and
              may reject unrecognized values.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
            type: string
          kind:
            description: |-
              Kind is a string value representing the REST resource this object represents.
              Servers may infer this from the endpoint the client submits requests to.
              Cannot be updated.
              In CamelCase.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
            type: string
          metadata:
            type: object
          spec:
            description: WorkloadProfileSpec defines the desired state of WorkloadProfile.
            properties:
              autoScalingConfig:
                description: |-
                  AutoScalingConfig configured here will override Pool's schedulingConfig
                  This field can not be fully supported in annotation, if user want to enable auto-scaling in annotation,
                  user can set tensor-fusion.ai/auto-limits|requests|replicas: 'true'
                properties:
                  autoSetLimits:
                    description: |-
                      layer 1 vertical auto-scaling, turbo burst to existing GPU cards quickly
                      VPA-like, aggregate metrics data <1m
                    properties:
                      enable:
                        type: boolean
                      evaluationPeriod:
                        type: string
                      extraTFlopsBufferRatio:
                        type: string
                      ignoredDeltaRange:
                        type: string
                      maxRatioToRequests:
                        description: the multiplier of requests, to avoid limit set
                          too high, like 5.0
                        type: string
                      prediction:
                        properties:
                          enable:
                            type: boolean
                          historyDataPeriod:
                            type: string
                          model:
                            type: string
                          predictionPeriod:
                            type: string
                        type: object
                      scaleUpStep:
                        type: string
                      targetResource:
                        description: target resource to scale limits, such as "tflops",
                          "vram", or "all" by default
                        type: string
                    type: object
                  autoSetReplicas:
                    description: |-
                      layer 2 horizontal auto-scaling, scale up to more GPU cards if max limits threshold hit
                      HPA-like, aggregate metrics data 1m-1h (when tf-worker scaled-up, should also trigger client pod's owner[Deployment etc.]'s replica increasing, check if KNative works)
                    properties:
                      enable:
                        type: boolean
                      evaluationPeriod:
                        type: string
                      scaleDownCoolDownTime:
                        type: string
                      scaleDownStep:
                        type: string
                      scaleUpCoolDownTime:
                        type: string
                      scaleUpStep:
                        type: string
                      targetTFlopsOfLimits:
                        type: string
                    type: object
                  autoSetRequests:
                    description: |-
                      layer 3 adjusting, to match the actual usage in the long run
                      Adjust baseline requests to match the actual usage in longer period, such as 1day - 2weeks
                    properties:
                      aggregationPeriod:
                        type: string
                      enable:
                        type: boolean
                      evaluationPeriod:
                        type: string
                      extraBufferRatio:
                        description: the request buffer ratio, for example actual
                          usage is 1.0, 10% buffer will be 1.1 as final preferred
                          requests
                        type: string
                      percentileForAutoRequests:
                        type: string
                      prediction:
                        properties:
                          enable:
                            type: boolean
                          historyDataPeriod:
                            type: string
                          model:
                            type: string
                          predictionPeriod:
                            type: string
                        type: object
                      targetResource:
                        description: target resource to scale requests, such as "tflops",
                          "vram", or "all" by default
                        type: string
                    type: object
                  scaleToZero:
                    description: |-
                      additional layer to save VRAM, auto-freeze memory and cool down to RAM and Disk
                      Hypervisor will monitor and trigger freeze of inactive workers, Operator should mark them as scaled-to-zero and release the GPU pool resources, don't scale down CPU client part, so that they can continue to serve the traffic or scale down by other auto-scaling solutions like KEDA/KNative
                    properties:
                      autoFreeze:
                        items:
                          properties:
                            enable:
                              type: boolean
                            freezeToDiskTTL:
                              type: string
                            freezeToMemTTL:
                              type: string
                            qos:
                              enum:
                              - low
                              - medium
                              - high
                              - critical
                              type: string
                          type: object
                        type: array
                      intelligenceWarmup:
                        properties:
                          enable:
                            type: boolean
                          historyDataPeriod:
                            type: string
                          model:
                            type: string
                          predictionPeriod:
                            type: string
                        type: object
                    type: object
                type: object
              gpuCount:
                description: The number of GPUs to be used by the workload, default
                  to 1
                type: integer
              gpuModel:
                description: GPUModel specifies the required GPU model (e.g., "A100",
                  "H100")
                type: string
              isLocalGPU:
                description: Schedule the workload to the same GPU server that runs
                  vGPU worker for best performance, default to false
                type: boolean
              poolName:
                type: string
              qos:
                description: Qos defines the quality of service level for the client.
                enum:
                - low
                - medium
                - high
                - critical
                type: string
              replicas:
                description: |-
                  If replicas not set, it will be dynamic based on pending Pod
                  If isLocalGPU set to true, replicas must be dynamic, and this field will be ignored
                format: int32
                type: integer
              resources:
                properties:
                  limits:
                    properties:
                      tflops:
                        anyOf:
                        - type: integer
                        - type: string
                        pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                        x-kubernetes-int-or-string: true
                      vram:
                        anyOf:
                        - type: integer
                        - type: string
                        pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                        x-kubernetes-int-or-string: true
                    required:
                    - tflops
                    - vram
                    type: object
                  requests:
                    properties:
                      tflops:
                        anyOf:
                        - type: integer
                        - type: string
                        pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                        x-kubernetes-int-or-string: true
                      vram:
                        anyOf:
                        - type: integer
                        - type: string
                        pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                        x-kubernetes-int-or-string: true
                    required:
                    - tflops
                    - vram
                    type: object
                required:
                - limits
                - requests
                type: object
              standaloneWorkerMode:
                description: This mode is only available when `is-local-gpu` set to
                  true, in this mode, TensorFusion will also inject vGPU worker into
                  init container, so that to achieve best performance, trade-off is
                  user might by-pass the vGPU worker and using physical GPU directly
                type: boolean
            type: object
          status:
            description: WorkloadProfileStatus defines the observed state of WorkloadProfile.
            type: object
        type: object
    served: true
    storage: true
    subresources:
      status: {}
