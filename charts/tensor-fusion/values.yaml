# Default values for tensor-fusion.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


# This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ""
fullnameOverride: ""
namespaceOverride: ""

#This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}

initialGpuNodeLabelSelector: "nvidia.com/gpu.present=true"

controller:
  # This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
  replicaCount: 1

  # This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/
  image:
    repository: tensorfusion/tensor-fusion-operator
    # Overrides the image tag whose default is the chart appVersion.
    tag: "latest"
  # This is for setting Kubernetes Annotations to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ 
  
  vectorAgentImage: docker.io/timberio/vector:latest-alpine

  podAnnotations: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchExpressions:
              - key: tensor-fusion.ai/component
                operator: In
                values:
                  - operator

  livenessProbe:
    httpGet:
      path: /healthz
      port: 8081
    initialDelaySeconds: 15
    periodSeconds: 20
    timeoutSeconds: 5
    failureThreshold: 5
  readinessProbe:
    httpGet:
      path: /readyz
      port: 8081
    initialDelaySeconds: 5
    periodSeconds: 15
    timeoutSeconds: 5
    failureThreshold: 2
  resources:
    requests:
      memory: 256Mi
      cpu: 50m
    limits:
      memory: 2Gi
      cpu: 2000m

  admissionWebhooks:
    failurePolicy: Fail
    secretName: tensor-fusion-webhook-secret
    patch:
      image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.0
greptime:
  isCloud: false
  host: greptimedb-standalone.greptimedb.svc.cluster.local
  port: 4002
  db: public
  installStandalone: true
  image:
    repository: docker.io/greptime/greptimedb
    tag: latest
  resources:
    requests:
      memory: 512Mi
      cpu: 100m
    limits:
      memory: 2Gi
      cpu: 200m

# greptime:
#   isCloud: true
#   host: your-instance.us-west-2.aws.greptime.cloud
#   user: "dummy"
#   db: "public"
#   password: "dummy"
#   port: 5001

agent:
  enrollToken: "token-from-cloud"
  agentId: 'org-from-cloud:env'
  cloudEndpoint: "wss://app.tensor-fusion.ai"
  
  image:
    repository: tensorfusion/tensor-fusion-agent
    tag: "latest"
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 4000m
      memory: 2Gi

# Only needed if your pool is running in Provisioned mode, and the cloud vendor doesn't support IRSA or any serviceAccount like zero-credential Auth approaches
cloudVendorCredentials:
  accessKey: "dummy"
  secretKey: "dummy"

alert:
  enabled: true
  image:
    repository: quay.io/prometheus/alertmanager
    tag: v0.28.1
  replicaCount: 1
  resources:
    requests:
      memory: 256Mi
      cpu: 50m
    limits:
      memory: 1Gi
      cpu: 1500m
  alertManagerConfig:
    global: {}
    receivers:
    - name: default-receiver
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
    templates:
    - /etc/alertmanager/*.tmpl

# KV structure config for other global configs
dynamicConfig:
  # retention period for metrics data
  metricsTTL: 30d

  # alert rules
  alertRules:
    - name: TFWorkerPodRestartCountIncreasing
      query: SQL
      threshold: 2
      evaluationInterval: 1m
      consecutiveCount: 1
      severity: P0
      summary: "TF Worker Pod Restart Count Increasing"
      description: "Worker pod {{ $labels.instance }} has restarted more than 2 times"
    
    # Worker TFlops throttled alert
    - name: TFWorkerTFlopsThrottled
      query: tf_worker_tflops_used > tf_worker_tflops_limit
      threshold: 0.95
      evaluationInterval: 15s
      consecutiveCount: 3
      severity: P1
      summary: "Worker TFlops Throttled"
      description: "Worker {{ $labels.instance }} is using more than 95% of its TFlops limit"
    
    # Worker VRAM switching too frequent alert
    - name: TFWorkerVRAMSwitchCountIncreasing
      query: tf_worker_vram_switch_count
      threshold: 2
      evaluationInterval: 2m
      consecutiveCount: 1
      severity: P1
      summary: "TF Worker Pod VRAM Switch Count Increasing"
      description: "Worker {{ $labels.instance }} has switched VRAM more than 2 times"
    
    # Worker can not scale up/scheduled alert
    - name: TFWorkerPendingScaleUp
      query: tf_pending_scale_up_worker_count
      threshold: 0
      evaluationInterval: 30s
      consecutiveCount: 1
      severity: P1
      summary: "Pending Scale Up Worker"
      description: "There are workers pending scale up"
    
    # Single GPU Alerts
    
    # GPU VRAM Full Alert
    - name: GPUVRAMFull
      query:  select memory_percentage from tf_gpu_usage where pool_name = {{ $labels.pool_name }} // TODO ID
      threshold: 0.97
      evaluationInterval: 30s
      consecutiveCount: 2
      severity: P1
      summary: "GPU VRAM Full"
      description: "GPU {{ $labels.instance }} has VRAM usage above 97%"
    
    # GPU TFlops Full Alert
    - name: GPUTFlopsFull
      query: tf_gpu_tflops_used / tf_gpu_tflops_total
      threshold: 0.97
      evaluationInterval: 30s
      consecutiveCount: 4
      severity: P1
      summary: "GPU TFlops Full"
      description: "GPU {{ $labels.instance }} has TFlops usage above 97%"
    
    # GPU Temperature alert
    - name: GPUTemperatureHigh
      query: tf_gpu_temperature_celsius
      threshold: 90
      evaluationInterval: 30s
      consecutiveCount: 3
      severity: P1
      summary: "GPU Temperature High"
      description: "GPU {{ $labels.instance }} has temperature above 90Â°C"
    
    # GPU Pool Alerts
    
    # Pool TFlops exhausted alert - Any GPU
    - name: PoolAnyGPUTFlopsExhaustedCritical
      query: min(tf_gpu_tflops_available / tf_gpu_tflops_total) by pool_id
      threshold: 0.05
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P0
      summary: "Pool Any GPU TFlops Exhausted Critical"
      description: "In pool {{ $labels.pool_id }}, at least one GPU has available TFlops below 5%"
    
    - name: PoolAnyGPUTFlopsExhaustedWarning
      query: min(tf_gpu_tflops_available / tf_gpu_tflops_total) by pool_id
      threshold: 0.10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Pool Any GPU TFlops Exhausted Warning"
      description: "In pool {{ $labels.pool_id }}, at least one GPU has available TFlops below 10%"
    
    # Pool TFlops exhausted alert - Total
    - name: PoolTotalTFlopsExhaustedCritical
      query: sum(tf_gpu_tflops_available) / sum(tf_gpu_tflops_total) by pool_id
      threshold: 0.05
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P0
      summary: "Pool Total TFlops Exhausted Critical"
      description: "In pool {{ $labels.poolName }}, total available TFlops below 5%"
    
    - name: PoolTotalTFlopsExhaustedWarning
      query: sum(tf_gpu_tflops_available) / sum(tf_gpu_tflops_total) by pool_id
      threshold: 0.10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Pool Total TFlops Exhausted Warning"
      description: "In pool {{ $labels.poolName }}, total available TFlops below 10%"
    
    # Pool VRAM exhausted alert - Any GPU
    - name: PoolAnyGPUVRAMExhausted
      query: min(tf_gpu_vram_available_bytes) by pool_id
      threshold: 0
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Pool Any GPU VRAM Exhausted"
      description: "In pool {{ $labels.poolName }}, at least one GPU has 0 available VRAM"
    
    # Pool VRAM exhausted alert - Total
    - name: PoolTotalVRAMExhaustedCritical
      query: sum(tf_gpu_vram_available_bytes) / sum(tf_gpu_vram_total_bytes) by pool_id
      threshold: 0.05
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P0
      summary: "Pool Total VRAM Exhausted Critical"
      description: "In pool {{ $labels.poolName }}, total available VRAM below 5%"
    
    - name: PoolTotalVRAMExhaustedWarning
      query: sum(tf_gpu_vram_available_bytes) / sum(tf_gpu_vram_total_bytes) by pool_id
      threshold: 0.10
      evaluationInterval: 1m
      consecutiveCount: 2
      severity: P1
      summary: "Pool Total VRAM Exhausted Warning"
      description: "In pool {{ $labels.poolName }}, total available VRAM below 10%"
    
    # Empty GPU Alert
    - name: EmptyGPU
      query: tf_gpu_tflops_available == tf_gpu_tflops_total and tf_gpu_vram_available_bytes == tf_gpu_vram_total_bytes
      threshold: true
      evaluationInterval: 1m
      consecutiveCount: 3
      severity: P2
      summary: "Empty GPU"
      description: "GPU {{ $labels.instance }} has been completely unused"
